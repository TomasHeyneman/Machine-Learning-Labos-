{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "source": [
    "# Supervised learning- regression- knn and penalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "source": [
    "# 0. Loading packages and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomas\\AppData\\Local\\Temp\\ipykernel_23688\\2495569210.py:7: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n",
      "  plt.style.use('seaborn-darkgrid')\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn-darkgrid')\n",
    "\n",
    "df = pd.read_csv('insurance_cleaned.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "source": [
    "## 0.1. Train/test-split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomas\\AppData\\Local\\Temp\\ipykernel_23688\\1196696295.py:7: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X = df_shuffle.drop(['charges'],1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from random import Random\n",
    "\n",
    "df_shuffle = df.sample(frac=1, random_state=40)\n",
    "\n",
    "X = df_shuffle.drop(['charges'],1)\n",
    "y = df_shuffle['charges']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "num = ['age','bmi','children']\n",
    "scaler = StandardScaler()\n",
    "X_train_stand = X_train.copy()\n",
    "X_test_stand = X_test.copy()\n",
    "X_train_stand[num] = scaler.fit_transform(X_train[num])\n",
    "X_test_stand[num] = scaler.transform(X_test[num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>bmi</th>\n",
       "      <th>children</th>\n",
       "      <th>sex_female</th>\n",
       "      <th>sex_male</th>\n",
       "      <th>smoker_no</th>\n",
       "      <th>smoker_yes</th>\n",
       "      <th>region_northeast</th>\n",
       "      <th>region_northwest</th>\n",
       "      <th>region_southeast</th>\n",
       "      <th>region_southwest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>-0.870587</td>\n",
       "      <td>-1.225955</td>\n",
       "      <td>-0.901777</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1060</th>\n",
       "      <td>-1.084879</td>\n",
       "      <td>0.234041</td>\n",
       "      <td>-0.901777</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555</th>\n",
       "      <td>-0.799156</td>\n",
       "      <td>-1.111253</td>\n",
       "      <td>0.740608</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1155</th>\n",
       "      <td>-0.227711</td>\n",
       "      <td>-1.384080</td>\n",
       "      <td>1.561801</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>-1.227741</td>\n",
       "      <td>1.461355</td>\n",
       "      <td>-0.901777</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           age       bmi  children  sex_female  sex_male  smoker_no  \\\n",
       "217  -0.870587 -1.225955 -0.901777           0         1          1   \n",
       "1060 -1.084879  0.234041 -0.901777           0         1          1   \n",
       "555  -0.799156 -1.111253  0.740608           0         1          1   \n",
       "1155 -0.227711 -1.384080  1.561801           1         0          1   \n",
       "888  -1.227741  1.461355 -0.901777           0         1          1   \n",
       "\n",
       "      smoker_yes  region_northeast  region_northwest  region_southeast  \\\n",
       "217            0                 0                 0                 1   \n",
       "1060           0                 0                 0                 1   \n",
       "555            0                 0                 0                 0   \n",
       "1155           0                 1                 0                 0   \n",
       "888            0                 0                 0                 0   \n",
       "\n",
       "      region_southwest  \n",
       "217                  0  \n",
       "1060                 0  \n",
       "555                  1  \n",
       "1155                 0  \n",
       "888                  1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_stand.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1070 entries, 217 to 52\n",
      "Data columns (total 11 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   age               1070 non-null   int64  \n",
      " 1   bmi               1070 non-null   float64\n",
      " 2   children          1070 non-null   int64  \n",
      " 3   sex_female        1070 non-null   int64  \n",
      " 4   sex_male          1070 non-null   int64  \n",
      " 5   smoker_no         1070 non-null   int64  \n",
      " 6   smoker_yes        1070 non-null   int64  \n",
      " 7   region_northeast  1070 non-null   int64  \n",
      " 8   region_northwest  1070 non-null   int64  \n",
      " 9   region_southeast  1070 non-null   int64  \n",
      " 10  region_southwest  1070 non-null   int64  \n",
      "dtypes: float64(1), int64(10)\n",
      "memory usage: 100.3 KB\n"
     ]
    }
   ],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "source": [
    "# 1. Decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "source": [
    "We start with a simple decision tree, but use cross-validation with regard to the maximum depth to find the best model. Since there are 1070 observation, the depth can't be higher than 11."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor  \n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "depth = np.arange(1,11)    # This will give an array of numbers between 1 and 10\n",
    "cv_scores = []\n",
    "sd_scores = []\n",
    "# perform 5-fold cross validation on the  possible values for the radius (bandwith)\n",
    "for d in depth:\n",
    "    dec_tree = DecisionTreeRegressor(random_state = 0, max_depth=d)  \n",
    "    scores = cross_val_score(dec_tree, X_train_stand, y_train,  cv=5)\n",
    "    cv_scores.append(scores.mean())\n",
    "    sd_scores.append(np.sqrt(scores.var())/np.sqrt(5))\n",
    " \n",
    "max_value = max(cv_scores)\n",
    "max_index = cv_scores.index(max_value)\n",
    "\n",
    "plt.plot(depth, cv_scores)\n",
    "plt.xlabel('depth')\n",
    "plt.ylabel('R^2')\n",
    "\n",
    "plt.show()\n",
    "print('The best depth is', depth[max_index])\n",
    "print('The validated score is', max_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "source": [
    "5 is the best model. The model that is more simple (a depth of 3), is above the underlimit, so we choose 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import tree  \n",
    "dec_tree = DecisionTreeRegressor(random_state = 0, max_depth=depth[max_index])  \n",
    "dec_tree.fit(X_train_stand, y_train) \n",
    "print(dec_tree.score(X_train_stand, y_train) )\n",
    "print(max_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "source": [
    "# 2. Random forest\n",
    "Let's see if we can improve this by using random forests. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "criterion =['squared_error','absolute_error']\n",
    "n_estimators = [int(x) for x in np.linspace(start = 100, stop = 500, num = 400)]\n",
    "max_features = ['auto', 'sqrt', 'log2']\n",
    "max_depth = [int(x) for x in np.linspace(1, 10, num = 10)]\n",
    "min_samples_split = [int(x) for x in np.linspace(2, 10, num = 9)]\n",
    "min_samples_leaf = [int(x) for x in np.linspace(1, 10, num = 10)]\n",
    "max_leaf_nodes = [int(x) for x in np.linspace(10, 500, num = 490)]\n",
    "\n",
    "# create the random grid to search for best hyperparameters\n",
    "random_grid = {'criterion': criterion,\n",
    "               'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'max_leaf_nodes': max_leaf_nodes}\n",
    "\n",
    "# then do cross-validatoin\n",
    "rf = RandomForestRegressor()\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, \n",
    "                               cv = 3, verbose=5, random_state=42, n_jobs=-1)\n",
    "# n_jobs=-1 to run as many models  parallel as possible\n",
    "rf_random.fit(X_train_stand, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "print(rf_random.score(X_train_stand, y_train)) # training score\n",
    "print(rf_random.best_score_)  # validation score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "out = pd.DataFrame(rf_random.cv_results_)\n",
    "out.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "xlabel_names = ['param_max_depth','param_min_samples_split','param_min_samples_leaf','param_n_estimators',\n",
    "                'param_max_features', 'param_max_leaf_nodes', 'param_criterion']\n",
    "\n",
    "fig, axs = plt.subplots(3,3, figsize=(20,10))\n",
    "\n",
    "axs[0,0].scatter(out['param_max_depth'], out['mean_test_score'], c='blue');\n",
    "axs[0,0].set_title('max_depth')\n",
    "\n",
    "axs[0,1].scatter(out['param_min_samples_split'], out['mean_test_score'], c='blue');\n",
    "axs[0,1].set_title('min_samples_split')\n",
    "\n",
    "axs[0,2].scatter(out['param_min_samples_leaf'], out['mean_test_score'], c='blue');\n",
    "axs[0,2].set_title('min_samples_leaf')\n",
    "\n",
    "axs[1,0].scatter(out['param_n_estimators'], out['mean_test_score'], c='blue');\n",
    "axs[1,0].set_title('n_estimators')\n",
    "\n",
    "axs[1,1].scatter(out['param_max_features'], out['mean_test_score'], c='blue');\n",
    "axs[1,1].set_title('max_features')\n",
    "\n",
    "axs[1,2].scatter(out['param_max_leaf_nodes'], out['mean_test_score'], c='blue');\n",
    "axs[1,2].set_title('max_leaf_nodes')\n",
    "\n",
    "axs[2,0].scatter(out['param_criterion'], out['mean_test_score'], c='blue');\n",
    "axs[2,0].set_title('criterion')\n",
    "\n",
    "for ax in axs.flat:\n",
    "    ax.set(ylabel='r_squared')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.scatter(out['param_max_depth'], out['mean_test_score'], c='blue');\n",
    "ax.set_xlabel(\"max_depth\");\n",
    "ax.set_ylabel(\"r_squared\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "source": [
    "We can see that the best depth is 7, but 6, 9 and 10 do not perform much worse. However, we prefer models that are less complex. Here, that means models that are less deep. So, we chose 7 and 6 to go in the grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.scatter(out['param_min_samples_split'], out['mean_test_score'], c='blue');\n",
    "ax.set_xlabel(\"min_samples_split\");\n",
    "ax.set_ylabel(\"r_squared\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "source": [
    "We can see that the best value is 10, but 2,5,6, 8,9 and 10 do not perform much worse. However, we prefer models that are less complex. Here, that means models that have a higher min number of samples to allow splitting. So, we chose 10,11 and 12 to go in the grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.scatter(out['param_min_samples_leaf'], out['mean_test_score'], c='blue');\n",
    "ax.set_xlabel(\"min_samples_leaf\");\n",
    "ax.set_ylabel(\"r_squared\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "source": [
    "5 leads to the best R^2, but 8 and 10 are not that much worse. We prefer a higher number of samples in a leaf (less complex), thus we include 5, 8 and 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.scatter(out['param_n_estimators'], out['mean_test_score'], c='blue');\n",
    "ax.set_xlabel(\"n_estimators\");\n",
    "ax.set_ylabel(\"r_squared\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "source": [
    "The best value was with 236 trees. Let's add  234,235, 236,237 and 238 to the grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.scatter(out['param_max_features'], out['mean_test_score'], c='blue');\n",
    "ax.set_xlabel(\"max_features\");\n",
    "ax.set_ylabel(\"r_squared\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "source": [
    "Auto clearly wins here, so let's just go with that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.scatter(out['param_max_leaf_nodes'], out['mean_test_score'], c='blue');\n",
    "ax.set_xlabel(\"max_leaf_nodes\");\n",
    "ax.set_ylabel(\"r_squared\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "source": [
    "The best max_leaf_nodes was 410, but it all seems very similar. Let's just not restrict this in the grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.scatter(out['param_criterion'], out['mean_test_score'], c='blue');\n",
    "ax.set_xlabel(\"criterion\");\n",
    "ax.set_ylabel(\"r_squared\");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "source": [
    "mae (absolute error) had the best score. we will include only mae.\n",
    "\n",
    "Note: add more values to your grid for the project. I really limited myself here, so the model would not run too long for the demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "criterion =['absolute_error']\n",
    "n_estimators = [234,235, 236,237,238]\n",
    "max_features = ['auto']\n",
    "max_depth = [6,7]\n",
    "min_samples_split = [10,11,12]\n",
    "min_samples_leaf = [5,8,10]\n",
    "\n",
    "# create the random grid to search for best hyperparameters\n",
    "grid = {'criterion': criterion,\n",
    "               'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf}\n",
    "\n",
    "# then do cross-validatoin\n",
    "rf = RandomForestRegressor()\n",
    "rf_grid = GridSearchCV(estimator = rf, param_grid = grid,\n",
    "                               cv = 5, verbose=2,  n_jobs=-1)\n",
    "# n_jobs=-1 to run as many models  parallel as possible\n",
    "rf_grid.fit(X_train_stand, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "print(rf_grid.score(X_train_stand, y_train)) \n",
    "print(rf_grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "rf_grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "source": [
    "Not a lot has changed, the validation score improved a bit.\n",
    "We retrain the model with the best parameters on the whole training set, before we take a look at whitening the black box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "params = rf_grid.best_params_\n",
    "rf_gridBest = RandomForestRegressor(**params)\n",
    "rf_gridBest.fit(X_train_stand, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "source": [
    "# 3. Whitening the black box\n",
    "\n",
    "Random forest is good all-round algorithm. It may not always be the very best, but it will almost always lead to a very reasonable performance, whithout a lot of effort. However, in linear regression we had the beta-parameters, which are easy to interpret. In random forest, it becomes a lot harder to interpret your results. This is why machine learning is often referred to as a black box. However, there are ways to whiten this black box. We will try to find an anwer for three questions\n",
    "1. Which features are important?\n",
    "2. What is the influence of a certain feature?\n",
    "3. Why is a prediction what it is?\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "source": [
    "## 3.1 Which features are important?\n",
    "\n",
    "You can ask for the variable importance by using the function .feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "list(zip(X_train.columns, rf_gridBest.feature_importances_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "source": [
    "This is not easy to read, so it is better to plot this. But first, let's check if the sum of all feature importances is indeed 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "rf_gridBest.feature_importances_.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "\n",
    "feat_importances = pd.Series(rf_gridBest.feature_importances_, index=X.columns)\n",
    "feat_importances.nlargest(11).plot(kind='barh')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "source": [
    "\n",
    "So, the five features that seem to have the most influence in our algorithm are the number of children, bmi, not smoking, smoking and age.\n",
    "\n",
    "But in what is the direction of this influence? Here, you might guess it from the context ( being older will probably result in higher medical costs), but be carefull with making assumptions like that. There is always a chance that you are wrong. So you have to check the directions of the influences yourself."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "source": [
    "Another way to check the importance of features is to calculate the permutation importance using the eli5-package. This permutes the values of each feature one by one and checks how it changes model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "source": [
    "## 3.2 What is the influence of a certain feature?\u000b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "source": [
    "Now that we know which are the important features, we can try to figure our how they are related to the outcome.  To this end, we build a regression on a set of predictions. It really doesn't matter in which set you calculate this, as long as it is representative of the population you want to discuss. Here, we do it just for the train set.\n",
    "\n",
    "The first step is to get the predictions from the random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "pred_train =rf_gridBest.predict(X_train_stand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "source": [
    "We then perform one univariate linear regressions per feature on the predicted values!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "source": [
    "We use the standardized features and also standardize the predictions, to make the interpretation easier (will be a number between -1 and 1). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "scaler2 = StandardScaler()\n",
    "X_train_stand2 = X_train.copy()\n",
    "X_train_stand2 = scaler.fit_transform(X_train)\n",
    "\n",
    "X = pd.DataFrame(X_train_stand2)\n",
    "X.columns = X_train.columns\n",
    "predictors =  X_train.columns\n",
    "\n",
    "\n",
    "y = np.array((pred_train-pred_train.mean())/np.sqrt(pred_train.var())).reshape(-1, 1)\n",
    "\n",
    "for i in np.arange(1,(X.shape[1])):\n",
    "    reg = LinearRegression().fit(X[[predictors[i]]], y)\n",
    "    beta_help = pd.Series(reg.coef_[0])\n",
    "    names_help = pd.Series(predictors[i])\n",
    "    beta = pd.concat([beta,beta_help], axis=0)\n",
    "    names = pd.concat([names,names_help], axis=0)\n",
    "betas = pd.concat([names,beta],axis=1)\n",
    "betas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "source": [
    "\n",
    "We can see that how older you are, the higher the costs; having more children lead to higher costs; being female leads to lower costs, while being male then ofcourse leads to higher costs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "source": [
    "## 3.3 Why is a prediction what it is?\n",
    "\n",
    "Now say we have a prediction that seems odd (e.g the prediction is far of the true value). You would like to know how your model came to that prediction.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "import lime\n",
    "import lime.lime_tabular\n",
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "\n",
    "\n",
    "# creating the explainer function\n",
    "explainer = LimeTabularExplainer(X_train_stand.values, mode=\"regression\", feature_names=X_train.columns)\n",
    "\n",
    "# storing a new observation\n",
    "i = 100\n",
    "X_test_stand = pd.DataFrame(X_test_stand)\n",
    "X_test_stand.columns = X_train.columns\n",
    "\n",
    "X_observation = X_test_stand.iloc[[i], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "rf_gridBest.predict(X_observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "# explanation using the random forest model\n",
    "explanation = explainer.explain_instance(X_observation.values[0], rf_gridBest.predict)\n",
    "explanation.show_in_notebook(show_table=True, show_all=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "source": [
    "We can see that the prediction for this point is 9.78 (middle of the left bar). By permuting the values of the different features, the prediction ranged from 7.29 to 10.79. In the table on the right, we can see the point itself. In the graph in the middle, you can see how the features influence the point. So, if you are a smoker (smoker_yes > 0), this will have a positive impact on the prediction (the prediction will be higher with 0.80). If the standardized age is lower than -0.87, the prediction will be lower with 0.80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "source": [
    "# 4. Gradient boosting\n",
    "Let's add one last algorithm for regression. After this, we will move on to classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "\n",
    "n_estimators = [int(x) for x in np.linspace(start = 100, stop = 500, num = 50)]\n",
    "max_features = ['auto', 'sqrt', 'log2']\n",
    "max_depth = [int(x) for x in np.linspace(1, 10, num = 10)]\n",
    "min_samples_split = [int(x) for x in np.linspace(2, 10, num = 9)]\n",
    "min_samples_leaf = [int(x) for x in np.linspace(1, 10, num = 10)]\n",
    "max_leaf_nodes = [int(x) for x in np.linspace(10, 500, num = 50)]\n",
    "loss = ['ls','lad','huber','quantile']\n",
    "learning_rate = [round(x,5) for x in np.logspace(start = -3, stop = -0.01, num = 20)]\n",
    "\n",
    "# create the random grid to search for best hyperparameters\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'max_leaf_nodes': max_leaf_nodes,\n",
    "               'loss': loss,\n",
    "               'learning_rate': learning_rate}\n",
    "\n",
    "# then do cross-validatoin\n",
    "gbm = GradientBoostingRegressor()\n",
    "gbm_random = RandomizedSearchCV(estimator = gbm, param_distributions = random_grid, n_iter = 100, \n",
    "                               cv = 3, verbose=2, random_state=42, n_jobs=-1)\n",
    "gbm_random.fit(X_train_stand, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "print(gbm_random.score(X_train_stand, y_train))\n",
    "print(gbm_random.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "gbm_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "out = pd.DataFrame(gbm_random.cv_results_)\n",
    "out.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "out.mean_test_score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "out2 = out\n",
    "\n",
    "\n",
    "xlabel_names = ['param_max_depth','param_min_samples_split','param_min_samples_leaf','param_n_estimators',\n",
    "                'param_max_features', 'param_max_leaf_nodes']\n",
    "\n",
    "fig, axs = plt.subplots(3,3, figsize=(20,10))\n",
    "\n",
    "axs[0,0].scatter(out2['param_max_depth'], out2['mean_test_score'], c='blue');\n",
    "axs[0,0].set_title('max_depth')\n",
    "\n",
    "axs[0,1].scatter(out2['param_min_samples_split'], out2['mean_test_score'], c='blue');\n",
    "axs[0,1].set_title('min_samples_split')\n",
    "\n",
    "axs[0,2].scatter(out2['param_min_samples_leaf'], out2['mean_test_score'], c='blue');\n",
    "axs[0,2].set_title('min_samples_leaf')\n",
    "\n",
    "axs[1,0].scatter(out2['param_n_estimators'], out2['mean_test_score'], c='blue');\n",
    "axs[1,0].set_title('n_estimators')\n",
    "\n",
    "axs[1,1].scatter(out2['param_max_features'], out2['mean_test_score'], c='blue');\n",
    "axs[1,1].set_title('max_features')\n",
    "\n",
    "axs[1,2].scatter(out2['param_max_leaf_nodes'], out2['mean_test_score'], c='blue');\n",
    "axs[1,2].set_title('max_leaf_nodes')\n",
    "\n",
    "axs[2,1].scatter(out2['param_learning_rate'], out2['mean_test_score'], c='blue');\n",
    "axs[2,1].set_title('learning_rate')\n",
    "\n",
    "axs[2,2].scatter(out2['param_loss'], out2['mean_test_score'], c='blue');\n",
    "axs[2,2].set_title('loss')\n",
    "\n",
    "\n",
    "for ax in axs.flat: ax.set(ylabel='r_squared')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "source": [
    "It is hard to see anything on these graphs, because of some very low scores. We will zoom in on the R^2 above 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "out2 = out[out.mean_test_score > 0.7]\n",
    "\n",
    "\n",
    "xlabel_names = ['param_max_depth','param_min_samples_split','param_min_samples_leaf','param_n_estimators',\n",
    "                'param_max_features', 'param_max_leaf_nodes', 'param_criterion']\n",
    "\n",
    "fig, axs = plt.subplots(3,3, figsize=(20,10))\n",
    "\n",
    "axs[0,0].scatter(out2['param_max_depth'], out2['mean_test_score'], c='blue');\n",
    "axs[0,0].set_title('max_depth')\n",
    "\n",
    "axs[0,1].scatter(out2['param_min_samples_split'], out2['mean_test_score'], c='blue');\n",
    "axs[0,1].set_title('min_samples_split')\n",
    "\n",
    "axs[0,2].scatter(out2['param_min_samples_leaf'], out2['mean_test_score'], c='blue');\n",
    "axs[0,2].set_title('min_samples_leaf')\n",
    "\n",
    "axs[1,0].scatter(out2['param_n_estimators'], out2['mean_test_score'], c='blue');\n",
    "axs[1,0].set_title('n_estimators')\n",
    "\n",
    "axs[1,1].scatter(out2['param_max_features'], out2['mean_test_score'], c='blue');\n",
    "axs[1,1].set_title('max_features')\n",
    "\n",
    "axs[1,2].scatter(out2['param_max_leaf_nodes'], out2['mean_test_score'], c='blue');\n",
    "axs[1,2].set_title('max_leaf_nodes')\n",
    "\n",
    "\n",
    "axs[2,1].scatter(out2['param_learning_rate'], out2['mean_test_score'], c='blue');\n",
    "axs[2,1].set_title('learning_rate')\n",
    "\n",
    "axs[2,2].scatter(out2['param_loss'], out2['mean_test_score'], c='blue');\n",
    "axs[2,2].set_title('loss')\n",
    "\n",
    "\n",
    "for ax in axs.flat: ax.set(ylabel='r_squared')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "n_estimators = [172,173,174]\n",
    "max_features = ['auto']\n",
    "max_depth = [4,5]\n",
    "min_samples_split = [5,7]\n",
    "min_samples_leaf = [7,10]\n",
    "max_leaf_nodes = [129,130,131]\n",
    "learning_rate = [(x) for x in np.linspace(0.01, 0.05, num = 3)]\n",
    "loss = ['huber']\n",
    "\n",
    "# create the random grid to search for best hyperparameters\n",
    "grid = {\n",
    "               'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "            'max_leaf_nodes': max_leaf_nodes,\n",
    "               'learning_rate': learning_rate,\n",
    "               'loss':loss}\n",
    "\n",
    "# then do cross-validatoin\n",
    "gbm = GradientBoostingRegressor()\n",
    "gbm_grid = GridSearchCV(estimator = gbm, param_grid = grid,\n",
    "                               cv = 3, verbose=2,  n_jobs=-1)\n",
    "# n_jobs=-1 to run as many models  parallel as possible\n",
    "gbm_grid.fit(X_train_stand, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "gbm_grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "gbm_grid.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "source": [
    "This is the best validated score so far. We choose this model. We retrain the model on the whole training set and evaluate on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "params = gbm_grid.best_params_\n",
    "gbm_gridBest = GradientBoostingRegressor(**params)\n",
    "gbm_gridBest.fit(X_train_stand, y_train)\n",
    "print('R2: %.3f' % gbm_gridBest.score(X_train_stand, y_train)) # train score\n",
    "print('R2: %.3f' % gbm_gridBest.score(X_test_stand, y_test))  # test score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "source": [
    " We will end it here for supervised regression. There are many, many more algorithms, but you got the main idea of how to do machine learning for regression. You can now easily learn other algorithms if you want to. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "source": [
    "# 5. Adaboost & XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "adareg = AdaBoostRegressor(random_state = 123, n_estimators=10)\n",
    "adareg.fit(X_train_stand, y_train)\n",
    "pred = adareg.predict(X_test_stand)\n",
    "adareg_r2 = r2_score(y_test, pred)\n",
    "print(\"R2:\", adareg_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "residuals = y_test - pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(y_test, residuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "import xgboost as xg\n",
    "\n",
    "xgbr = xg.XGBRegressor(objective = 'reg:squarederror', n_estimators = 10, seed = 123)\n",
    "xgbr.fit(X_train_stand, y_train)\n",
    "pred = xgbr.predict(X_test_stand)\n",
    "xgb_r2 = r2_score(y_test, pred)\n",
    "print(\"R2:\", xgb_r2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "les4_demo_labo-TGZHoNia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [
       "c#",
       "C#"
      ],
      "languageName": "C#",
      "name": "csharp"
     },
     {
      "aliases": [
       "frontend"
      ],
      "languageName": null,
      "name": "vscode"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
