{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised learning- categorisation- logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Loading packages and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn-darkgrid')\n",
    "\n",
    "df = pd.read_csv('real_estate_cleaned.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the same dataset as for the regression setting. But now, we will not try to predict the class, but the price class. In the following code, I create two price classes \n",
    "+ 0 = all prices below 400 000 (or the log(prices) < 12.9 - remember that we used the log-transform when cleaning the data)\n",
    "+ 1 = all prices above 400 000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['price_class'] = 1\n",
    "#df['price_class'][df.tx_price > 12.9] = 2\n",
    "#df['price_class'][df.tx_price > 13.3] = 3\n",
    "\n",
    "df['price_class'] = 0\n",
    "df.loc[df.tx_price > 12.9, 'price_class'] = 1\n",
    "#df['price_class'][df.tx_price > 12.9] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['price_class','tx_price']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how many observations we have in each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['price_class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly imbalanced. We will see how to deal with this later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df.drop(['tx_price'],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we only did this for instruction reasons. You do not want to classify your outcome like this if you have continuous data. An algoritm will always be able to get more information out of the continuous data. Say you are only interested in knowing if a house is more expensive than 500 000 euro or not. Then it is a better strategy to predict the continuous prices and then classify after prediction). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1. Train/val/test-split\n",
    "We make a three-way split, because we need an extra hold-out set for the calibration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "X = df_new.drop(['price_class'],1)\n",
    "y = df_new['price_class']\n",
    "\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_trainval,y_trainval, test_size=0.2, random_state=81357)\n",
    "\n",
    "\n",
    "num_feat = ['beds', 'baths', 'sqft', 'year_built', 'lot_size', 'restaurants',\n",
    "       'groceries', 'nightlife', 'cafes', 'shopping', 'arts_entertainment',\n",
    "       'beauty_spas', 'active_life', 'median_age', 'married', 'college_grad',\n",
    "       'property_tax', 'insurance', 'median_school', 'num_schools', 'tx_year',\n",
    "       'school_score', 'tax_per_sqft']\n",
    "scaler = StandardScaler()\n",
    "X_train_stand = X_train.copy()\n",
    "X_trainval_stand = X_trainval.copy()\n",
    "X_val_stand = X_val.copy()\n",
    "\n",
    "X_test_stand = X_test.copy()\n",
    "X_train_stand[num_feat] = scaler.fit_transform(X_train_stand[num_feat])\n",
    "X_val_stand[num_feat] = scaler.transform(X_val_stand[num_feat])\n",
    "X_trainval_stand[num_feat] = scaler.transform(X_trainval_stand[num_feat])\n",
    "X_test_stand[num_feat] = scaler.transform(X_test_stand[num_feat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Logistic regression\n",
    " ## 1.1 Basic model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with the simple logistic regression model. The C-parameter (the inverse of the regularisation) is per default at 1, which means there is regularisation. To have no regularisation, we put C at a very high number (10 000). Since we have a validation set, we can evaluate on that one, whitout risking bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression(C=10000, max_iter=200) \n",
    "logreg.fit(X_train_stand, y_train)\n",
    "print(logreg.score(X_train_stand, y_train))\n",
    "print(logreg.score(X_val_stand, y_val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score object gives the accuracy for classification (while it gives the R2 for regression problems). The predictions can also be asked as before. You can choose between .predict(), which gives the predicted class and .predict_proba(), which gives the predicted chance for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_pred = logreg.predict(X_val_stand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_pred_prob = logreg.predict_proba(X_val_stand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install scikit-plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also take a look at the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix = confusion_matrix(y_val, y_val_pred)\n",
    "print(confusion_matrix)\n",
    "\n",
    "import scikitplot as skplt\n",
    "skplt.metrics.plot_confusion_matrix(y_val, y_val_pred, normalize=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could now calculate the quality measure you want yourself. Or you can get some specific quality metrics from a classification report.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_val, y_val_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ The micro-average is the accuracy.\n",
    "+ The macro average is the average over the three classes for each specific measure\n",
    "+ The weighted average is the same as the macro average, but weighted for the number of observations in each class.\n",
    "\n",
    "Notice the high recall for the first class, while the recall of the second class is very low. This might be because of the imbalanced nature of our dataset. There are a lot of observations in class 1. We should take this into account (but we will see how in the next class). We will first focus on calibration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "# calculate the fpr and tpr for all thresholds of the classification\n",
    "preds = y_val_pred_prob[:,1]\n",
    "fpr, tpr, threshold = metrics.roc_curve(y_val, preds)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Calibration\n",
    "\n",
    "Let's check if our model is wel calibrated, and if not, perform a calibration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import calibration_curve\n",
    "y_pred_val_prob = logreg.predict_proba(X_val_stand)\n",
    "\n",
    "fop, mpv = calibration_curve(y_val,y_pred_val_prob[:,1],n_bins=10)\n",
    "\n",
    "plt.plot(mpv,fop, marker='o', linewidth=1, label='uncalibrated')\n",
    "plt.plot([0,1],[0,1],linestyle='--', label='perfectly calibrated')\n",
    "plt.xlabel('Mean predicted probability')\n",
    "plt.ylabel('Fraction of positives')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not good, but not teribly bad either (normally, logistic model tend to be well calibrated, calibration is more needed for tree-based methods, support vector machines...). We will do a isotonic regression to correct this. Note that we use the validationset to fit the isotonic regression. We set cv at 'prefit', because we have already fitted the model. So, the function assumes it can use the entire input dataset for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import CalibratedClassifierCV, calibration_curve\n",
    "isotonic = CalibratedClassifierCV(logreg, cv='prefit', method='isotonic')\n",
    "isotonic.fit(X_val_stand, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "y_val_pred_prob_c = isotonic.predict_proba(X_val_stand)\n",
    "\n",
    "fop, mpv = calibration_curve(y_val,y_pred_val_prob[:,1],n_bins=10)\n",
    "fop_c, mpv_c = calibration_curve(y_val,y_val_pred_prob_c[:,1],n_bins=10)\n",
    "\n",
    "plt.plot(mpv,fop, marker='o', linewidth=1, label='uncalibrated')\n",
    "plt.plot(mpv_c,fop_c, marker='o', linewidth=1, label='calibrated')\n",
    "\n",
    "plt.plot([0,1],[0,1],linestyle='--', label='perfectly calibrated')\n",
    "\n",
    "plt.xlabel('Mean predicted probability')\n",
    "plt.ylabel('Fraction of positives')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, this is much better (do not expect you calibration to end up to be perfect like this, especially not for other algorithms.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = logreg.predict(X_val_stand)\n",
    "y_pred_c = isotonic.predict(X_val_stand)\n",
    "\n",
    "print(classification_report(y_val, y_pred))\n",
    "print(classification_report(y_val, y_pred_c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a good recal for the first class, but a drop for the second class. You actually don't have to do the fitting of the model and the calibration seperately. You can do this at once (and it is better), using the CalibratedClassifierCV, but now you give a value for CV (e.g 3 or 5) instead of 'prefit'. A cross-validation is performed, where the train-set is split into 2, one for training and one for calibration. Since we want to compare several algorithms, we keep the validation dataset seperate to evaluate on (if you choose one algorithm, you can use the complete train_val dataset to do training and calibration on). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import CalibratedClassifierCV, calibration_curve\n",
    "logreg2 = LogisticRegression(C=10000, max_iter=1000) \n",
    "isotonic2 = CalibratedClassifierCV(logreg2, cv=3, method='isotonic')\n",
    "isotonic2.fit(X_train_stand, y_train)\n",
    "\n",
    "\n",
    "# The calibration plot\n",
    "y_val_pred_prob_c2 = isotonic2.predict_proba(X_val_stand)\n",
    "\n",
    "fop, mpv = calibration_curve(y_val,y_pred_val_prob[:,1],n_bins=10)\n",
    "fop_c, mpv_c = calibration_curve(y_val,y_val_pred_prob_c2[:,1],n_bins=10)\n",
    "\n",
    "plt.plot(mpv,fop, marker='o', linewidth=1, label='uncalibrated')\n",
    "plt.plot(mpv_c,fop_c, marker='o', linewidth=1, label='calibrated')\n",
    "plt.plot([0,1],[0,1],linestyle='--', label='perfectly calibrated')\n",
    "\n",
    "plt.xlabel('Mean predicted probability')\n",
    "plt.ylabel('Fraction of positives')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "y_pred_c2 = isotonic2.predict(X_val_stand)\n",
    "print(classification_report(y_val, y_pred_c))\n",
    "print(classification_report(y_val, y_pred_c2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this calibration seems less good in the plot, it is a much more realistic picture. It isn't perfect on the line, because there have been 3 seperate calibration, one for each test-fold within the cross-validation, resulting in three models. At the end, the predicted chance is the average of the three models. This is an example of ensemble learning.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Imbalanced data\n",
    "The dataset is imbalanced: there are a lot of observations in class 1.We should take this into account. We could use over- or undersampling. But logistic regression has a built-in argument to do class-weight balancing. So, we will use that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import CalibratedClassifierCV, calibration_curve\n",
    "logreg_balanced = LogisticRegression(C=10000, class_weight='balanced', max_iter=1000) \n",
    "isotonic3 = CalibratedClassifierCV(logreg_balanced, cv=3, method='isotonic')\n",
    "isotonic3.fit(X_train_stand, y_train)\n",
    "\n",
    "# The calibration plot\n",
    "y_val_pred_prob_c3 = isotonic3.predict_proba(X_val_stand)\n",
    "\n",
    "fop, mpv = calibration_curve(y_val,y_pred_val_prob[:,1],n_bins=10)\n",
    "fop_c, mpv_c = calibration_curve(y_val,y_val_pred_prob_c2[:,1],n_bins=10)\n",
    "fop_c3, mpv_c3 = calibration_curve(y_val,y_val_pred_prob_c3[:,1],n_bins=10)\n",
    "\n",
    "plt.plot(mpv,fop, marker='o', linewidth=1, label='uncalibrated')\n",
    "plt.plot(mpv_c,fop_c, marker='o', linewidth=1, label='calibrated')\n",
    "plt.plot(mpv_c3,fop_c3, marker='o', linewidth=1, label='calibrated balanced')\n",
    "\n",
    "plt.plot([0,1],[0,1],linestyle='--', label='perfectly calibrated')\n",
    "\n",
    "plt.xlabel('Mean predicted probability')\n",
    "plt.ylabel('Fraction of positives')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "y_pred_c3 = isotonic3.predict(X_val_stand)\n",
    "print(classification_report(y_val, y_pred_c2))\n",
    "print(classification_report(y_val, y_pred_c3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the recall of the second class improves a bit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Adding polynomials and penalisation\n",
    "\n",
    "Let's add polynomials, but immediately do the penalisation as well (since sklearn does this automatically anyway, using l2). We will go up to the 3th order of polynomials and then regulate it back down using penalisation (you could go to a higher order, but I choose 3 to keep the computation time low for instruction reasons). If you have more time to run the model, try it out and see what happens if you increase the order.\n",
    "\n",
    "We will also use cross-validation to find the optimal penalisation factor C. We will train on accuracy, since we don't really have a preference for recall or precision here. sklearn picks accuracy automatically, so you actually don't need to specify this, but just to show how to do this in case you want another metric, I show how.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "prec_scorer = make_scorer(metrics.accuracy_score)\n",
    "# other possibilities are metrics.recall_score, metrics.average_precision, ...\n",
    "\n",
    "C = [round(x,5) for x in np.linspace(start = 0.0001, stop = 10, num = 1000)]\n",
    "#penalty = ['l1', 'l2']\n",
    "class_weight = ['balanced']\n",
    "random_grid = {'C': C,    \n",
    "               'class_weight': class_weight}\n",
    "\n",
    "#Designing polynomial features\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly = PolynomialFeatures(degree=3)\n",
    "X_train_poly = poly.fit_transform(X_train_stand)\n",
    "X_val_poly = poly.fit_transform(X_val_stand)\n",
    "X_test_poly = poly.transform(X_test_stand)\n",
    "\n",
    "# The object to fit the model\n",
    "logreg = LogisticRegression( max_iter=1000) \n",
    "\n",
    "# object for the randomised search\n",
    "log_random = RandomizedSearchCV(estimator = logreg, param_distributions = random_grid,\n",
    "                                scoring=prec_scorer, n_iter = 100,\n",
    "                               cv = 3, verbose=2,  n_jobs=-1)\n",
    "\n",
    "#Fitting model (model+ calibration)\n",
    "log_random.fit(X_train_poly, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_random.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out2 = pd.DataFrame(log_random.cv_results_)\n",
    "\n",
    "#Have to replace the None values with something else, or it wont make the plots\n",
    "res = [i for i in range(len(out2['param_class_weight'])) if out2['param_class_weight'][i] == None] \n",
    "out2['param_class_weight'].iloc[res] = \"No\"\n",
    "\n",
    "xlabel_names = ['param_C','param_class_weight']\n",
    "\n",
    "\n",
    "plt.scatter(out2['param_C'], out2['mean_test_score'], c='blue');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that lower values for C (high regularisation) lead to higher accuracy. We will zoom in on these values. But, we also include the calibration now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "prec_scorer = make_scorer(metrics.accuracy_score)\n",
    "# other possibilities are metrics.recall_score, metrics.average_precision, ...\n",
    "\n",
    "C = [round(x,5) for x in np.linspace(start = 0, stop =4, num = 100)]\n",
    "class_weight = ['balanced']\n",
    "\n",
    "random_grid = {'C': C,\n",
    "               'class_weight': class_weight}\n",
    "\n",
    "# The object to fit the model\n",
    "logreg_balanced = LogisticRegression( max_iter=1000) \n",
    "\n",
    "# object for the randomised search\n",
    "log_grid = GridSearchCV(estimator = logreg_balanced, param_grid = random_grid,\n",
    "                                scoring=prec_scorer  , cv = 3, verbose=2,  n_jobs=-1)\n",
    "isotonic_log = CalibratedClassifierCV(log_grid, cv=3, method='isotonic')\n",
    "\n",
    "#Fitting model (model+ calibration)\n",
    "isotonic_log.fit(X_train_poly, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#isotonic_log.best_params_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This doesn't work, because we do not have 1 best model, but three, because of the cross-validation. So each fold choses its own best model and calibrates this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isotonic_log.score(X_val_poly, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred_poly = isotonic_log.predict(X_val_poly)\n",
    "print(classification_report(y_val, y_pred_poly))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "# calculate the fpr and tpr for all thresholds of the classification\n",
    "preds = y_val_pred_prob[:,1]\n",
    "preds_poly = isotonic_log.predict_proba(X_val_poly)[:,1]\n",
    "\n",
    "fpr, tpr, threshold = metrics.roc_curve(y_val, preds)\n",
    "fpr_p, tpr_p, threshold = metrics.roc_curve(y_val, preds_poly)\n",
    "\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "roc_auc_p = metrics.auc(fpr_p, tpr_p)\n",
    "\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'logistic: AUC =  %0.2f' % roc_auc)\n",
    "plt.plot(fpr_p, tpr_p, 'g', label = 'polynomial AUC = %0.2f' % roc_auc_p)\n",
    "\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ROC-curve shows that this final model is better than the simple logistic regression (which was already pretty good.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. knn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "n_neighbors = 4*np.arange(1,50)\n",
    "param_grid = {'n_neighbors': n_neighbors}\n",
    "knn = KNeighborsClassifier( )\n",
    "grid_search = GridSearchCV(estimator = knn, param_grid = param_grid, scoring=prec_scorer ,cv = 5,  verbose=2, n_jobs = -1)\n",
    "grid_search.fit(X_train_stand, y_train)\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out2 = pd.DataFrame(grid_search.cv_results_)\n",
    "xlabel_names = ['n_neighbors']\n",
    "plt.scatter(out2['param_n_neighbors'], out2['mean_test_score'], c='blue');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_knn = grid_search.predict(X_val_stand)\n",
    "print(classification_report(y_val, y_pred_knn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = y_val_pred_prob[:,1]\n",
    "preds_poly = isotonic_log.predict_proba(X_val_poly)[:,1]\n",
    "preds_knn = grid_search.predict_proba(X_val_stand)[:,1]\n",
    "\n",
    "\n",
    "fpr, tpr, threshold = metrics.roc_curve(y_val, preds)\n",
    "fpr_p, tpr_p, threshold = metrics.roc_curve(y_val, preds_poly)\n",
    "fpr_k, tpr_k, threshold = metrics.roc_curve(y_val, preds_knn)\n",
    "\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "roc_auc_p = metrics.auc(fpr_p, tpr_p)\n",
    "roc_auc_k = metrics.auc(fpr_k, tpr_k)\n",
    "\n",
    "\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'logistic: AUC =  %0.2f' % roc_auc)\n",
    "plt.plot(fpr_p, tpr_p, 'g', label = 'polynomial AUC = %0.2f' % roc_auc_p)\n",
    "plt.plot(fpr_k, tpr_k, 'r', label = 'knn AUC = %0.2f' % roc_auc_k)\n",
    "\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems that the polynomial logistic model performed better. However, we did not correct for the imbalance in this model. There is no function class_weight for knn. So we will have to correct the imbalance ourself. We will use hybrid sampling, where we first oversample using smote and then also undersample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "over = SMOTE(sampling_strategy=0.8, random_state=1703)\n",
    "under = RandomUnderSampler(sampling_strategy=1)\n",
    "pipeline = Pipeline([('o', over), ('u', under)])\n",
    "\n",
    "X_train_smote, y_train_smote = pipeline.fit_resample(X_train_stand.copy(), y_train.copy())\n",
    "X_val_smote, y_val_smote = pipeline.fit_resample(X_val_stand.copy(), y_val.copy())\n",
    "np.bincount(y_val_smote)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we already know lower values of k perform better, so I will focus on the lower values (up to 50 instead of 200 previously)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors = np.arange(1,50)\n",
    "param_grid = {'n_neighbors': n_neighbors}\n",
    "\n",
    "knn_b = KNeighborsClassifier( )\n",
    "grid_search_knn = GridSearchCV(estimator = knn, param_grid = param_grid, scoring=prec_scorer ,cv = 5,  verbose=2, n_jobs = -1)\n",
    "isotonic_knn = CalibratedClassifierCV(grid_search_knn, cv=3, method='isotonic')\n",
    "\n",
    "\n",
    "isotonic_knn.fit(X_train_smote, y_train_smote)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred_knn_b = isotonic_knn.predict(X_val_stand)\n",
    "print(classification_report(y_val, y_pred_knn_b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two results for both recall and precision are now a lot better balanced between the two classes. And the overal accuracy is also higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_knn_b = isotonic_knn.predict_proba(X_val_stand)[:,1]\n",
    "\n",
    "fpr, tpr, threshold = metrics.roc_curve(y_val, preds)\n",
    "fpr_p, tpr_p, threshold = metrics.roc_curve(y_val, preds_poly)\n",
    "fpr_k, tpr_k, threshold = metrics.roc_curve(y_val, preds_knn_b)\n",
    "\n",
    "\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "roc_auc_p = metrics.auc(fpr_p, tpr_p)\n",
    "roc_auc_k = metrics.auc(fpr_k, tpr_k)\n",
    "\n",
    "\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'logistic: AUC =  %0.2f' % roc_auc)\n",
    "plt.plot(fpr_p, tpr_p, 'g', label = 'polynomial AUC = %0.2f' % roc_auc_p)\n",
    "plt.plot(fpr_k, tpr_k, 'r', label = 'knn AUC = %0.2f' % roc_auc_k)\n",
    "\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the polynomial model still performs better overal. And according to the ROC-curve the model became worse. You see, you need to decide in advance what you will use to evaluate your model, or you will have conflicting messages. Let's move on to tree-based models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. GBM\n",
    "We won't do the simple decision tree or random forest, but just stick to the gradient boosting machine for the tree based methods. Again, it is very similar to the regression setting. Since GBM also does not have a class_weight function, we will use the smoted dataset again. \n",
    "\n",
    "We begin with a random search and end with a grid search that includes the calibration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " np.linspace(start = 0.001, stop = 1.5, num = 50)\n",
    " #np.linspace(1, 10, num = 20)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1000, num = 50)]\n",
    "max_features = ['auto', 'sqrt', 'log2']\n",
    "max_depth = [int(x) for x in np.linspace(1, 10, num = 10)]\n",
    "min_samples_split = [int(x) for x in np.linspace(2, 10, num = 9)]\n",
    "min_samples_leaf = [int(x) for x in np.linspace(1, 10, num = 10)]\n",
    "learning_rate = [round(x,5) for x in np.linspace(start = 0.001, stop = 1.5, num = 50)]\n",
    "\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'learning_rate': learning_rate,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf}\n",
    "gbm = GradientBoostingClassifier()\n",
    "gbm_random = RandomizedSearchCV(estimator = gbm, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=4872, n_jobs = -1)\n",
    "gbm_random.fit(X_train_smote, y_train_smote)\n",
    "gbm_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_gbm = gbm_random.predict(X_val_stand)\n",
    "print(classification_report(y_val, y_pred_gbm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, a high accuracy, with good values for precision and recall for both classes. This must be the best model we have seen so far. Let's see if we can improve it even more in the grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out2 = pd.DataFrame(gbm_random.cv_results_)\n",
    "\n",
    "xlabel_names = ['param_max_depth','param_min_samples_split','param_min_samples_leaf','param_n_estimators',\n",
    "                'param_learning_rate','param_max_features']\n",
    "\n",
    "fig, axs = plt.subplots(2,3, figsize=(20,10))\n",
    "\n",
    "axs[0,0].scatter(out2['param_max_depth'], out2['mean_test_score'], c='blue');\n",
    "axs[0,0].set_title('max_depth')\n",
    "\n",
    "axs[0,1].scatter(out2['param_min_samples_split'], out2['mean_test_score'], c='blue');\n",
    "axs[0,1].set_title('min_samples_split')\n",
    "\n",
    "axs[0,2].scatter(out2['param_min_samples_leaf'], out2['mean_test_score'], c='blue');\n",
    "axs[0,2].set_title('min_samples_leaf')\n",
    "\n",
    "axs[1,0].scatter(out2['param_n_estimators'], out2['mean_test_score'], c='blue');\n",
    "axs[1,0].set_title('n_estimators')\n",
    "\n",
    "axs[1,1].scatter(out2['param_learning_rate'], out2['mean_test_score'], c='blue');\n",
    "axs[1,1].set_title('learning_rate')\n",
    "\n",
    "axs[1,2].scatter(out2['param_max_features'], out2['mean_test_score'], c='blue');\n",
    "axs[1,2].set_title('max_features')\n",
    "\n",
    "for ax in axs.flat: ax.set(ylabel='r_squared')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x for x in np.linspace(start = 0.2, stop = 0.3, num = 10)]\n",
    "[x for x in (np.linspace(start = 0.2, stop = 0.3, num = 10), np.linspace(start = 1.2, stop = 1.3, num = 10) )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.calibration import CalibratedClassifierCV, calibration_curve\n",
    "\n",
    "n_estimators = [797,800,803]\n",
    "learning_rate = [0.2,0.21,0.22,0.23,0.24,0.25,0.26,0.27,0.28,0.29,0.30,\n",
    "                1.2,1.21,1.22,1.23,1.24,1.25,1.26,1.27,1.28,1.29,1.30]\n",
    "max_features = ['auto']\n",
    "max_depth = [3,7]\n",
    "min_samples_split = [9,10]\n",
    "min_samples_leaf = [5,9]\n",
    "                                            \n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'learning_rate': learning_rate,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf}\n",
    "gbm = GradientBoostingClassifier()\n",
    "gbm_grid = GridSearchCV(estimator = gbm, param_grid = random_grid,  cv = 3, verbose=2,  n_jobs = -1)\n",
    "\n",
    "isotonic_gbm = CalibratedClassifierCV(gbm_grid, cv=3, method='isotonic')\n",
    "isotonic_gbm.fit(X_train_smote, y_train_smote)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_gbm = isotonic_gbm.predict(X_val_stand)\n",
    "print(classification_report(y_val, y_pred_gbm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_gbm = isotonic_gbm.predict_proba(X_val_stand)[:,1]\n",
    "\n",
    "\n",
    "fpr, tpr, threshold = metrics.roc_curve(y_val, preds)\n",
    "fpr_p, tpr_p, threshold = metrics.roc_curve(y_val, preds_poly)\n",
    "fpr_k, tpr_k, threshold = metrics.roc_curve(y_val, preds_knn_b)\n",
    "fpr_g, tpr_g, threshold = metrics.roc_curve(y_val, preds_gbm)\n",
    "\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "roc_auc_p = metrics.auc(fpr_p, tpr_p)\n",
    "roc_auc_k = metrics.auc(fpr_k, tpr_k)\n",
    "roc_auc_g = metrics.auc(fpr_g, tpr_g)\n",
    "\n",
    "\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'logistic: AUC =  %0.2f' % roc_auc)\n",
    "plt.plot(fpr_p, tpr_p, 'g', label = 'polynomial AUC = %0.2f' % roc_auc_p)\n",
    "plt.plot(fpr_k, tpr_k, 'r', label = 'knn AUC = %0.2f' % roc_auc_k)\n",
    "plt.plot(fpr_g, tpr_g, 'y', label = 'gbm AUC = %0.2f' % roc_auc_g)\n",
    "\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ROC-curve also agrees this is the best model yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Support vector machines\n",
    "One final model. While this model is theoretically one of the harder ones, implementing it is rather simple, just do a cross-validation using four hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linspace(start = 1, stop = 10, num = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "C = [round(x,5) for x in np.linspace(start = 0.001, stop = 10, num = 50)]\n",
    "kernel = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "degree = [round(x) for x in np.linspace(start = 1, stop = 15, num = 15)]\n",
    "gamma = [round(x,5) for x in np.linspace(start = 0.001, stop = 1, num = 50)]\n",
    "#gamma = ['auto', 0.3,0.5, 0.7,0.9]\n",
    "\n",
    "random_grid = {'C': C,\n",
    "               'kernel': kernel,\n",
    "               'degree':degree,\n",
    "               'gamma': gamma}\n",
    "svc = SVC()\n",
    "svc_random = RandomizedSearchCV(estimator = svc, param_distributions = random_grid, n_iter =200, cv = 3, verbose=2, random_state=4872, n_jobs = -1)\n",
    "svc_random.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "print(svc_random.best_params_)\n",
    "params = svc_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_svm = svc_random.predict(X_val_stand)\n",
    "print(classification_report(y_val, y_pred_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out2 = pd.DataFrame(svc_random.cv_results_)\n",
    "\n",
    "xlabel_names = ['C',\n",
    "               'kernel',\n",
    "               'degree',\n",
    "               'gamma']\n",
    "\n",
    "fig, axs = plt.subplots(2,2, figsize=(20,10))\n",
    "\n",
    "axs[0,0].scatter(out2['param_C'], out2['mean_test_score'], c='blue');\n",
    "axs[0,0].set_title('C')\n",
    "\n",
    "axs[0,1].scatter(out2['param_kernel'], out2['mean_test_score'], c='blue');\n",
    "axs[0,1].set_title('kernel')\n",
    "\n",
    "axs[1,0].scatter(out2['param_degree'], out2['mean_test_score'], c='blue');\n",
    "axs[1,0].set_title('degree')\n",
    "\n",
    "axs[1,1].scatter(out2['param_gamma'], out2['mean_test_score'], c='blue');\n",
    "axs[1,1].set_title('gamma')\n",
    "\n",
    "for ax in axs.flat: ax.set(ylabel='r_squared')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rbf kernel is clearly the best. This means we nog longer need the degree-parameter, since this is only used by the polynomial kernel. gamma around 0.2 perform best, and for C values between 7 and 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "C = [round(x,5) for x in np.linspace(start = 7, stop = 8, num = 20)]\n",
    "kernel = ['rbf']\n",
    "gamma = [round(x,5) for x in np.linspace(start = 0.15, stop = 0.25, num = 20)]\n",
    "\n",
    "random_grid = {'C': C,\n",
    "               'kernel': kernel,\n",
    "               'gamma': gamma}\n",
    "svc = SVC()\n",
    "svc_grid = GridSearchCV(estimator = svc, param_grid = random_grid,  cv = 3, verbose=2,  n_jobs = -1)\n",
    "\n",
    "isotonic_svm = CalibratedClassifierCV(svc_grid, cv=3, method='isotonic')\n",
    "isotonic_svm.fit(X_train_smote, y_train_smote)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_svm = isotonic_svm.predict(X_val_stand)\n",
    "print(classification_report(y_val, y_pred_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_svm = isotonic_svm.predict_proba(X_val_stand)[:,1]\n",
    "\n",
    "\n",
    "fpr, tpr, threshold = metrics.roc_curve(y_val, preds)\n",
    "fpr_p, tpr_p, threshold = metrics.roc_curve(y_val, preds_poly)\n",
    "fpr_k, tpr_k, threshold = metrics.roc_curve(y_val, preds_knn)\n",
    "fpr_g, tpr_g, threshold = metrics.roc_curve(y_val, preds_gbm)\n",
    "fpr_s, tpr_s, threshold = metrics.roc_curve(y_val, preds_svm)\n",
    "\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "roc_auc_p = metrics.auc(fpr_p, tpr_p)\n",
    "roc_auc_k = metrics.auc(fpr_k, tpr_k)\n",
    "roc_auc_g = metrics.auc(fpr_g, tpr_g)\n",
    "roc_auc_s = metrics.auc(fpr_s, tpr_s)\n",
    "\n",
    "\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'logistic: AUC =  %0.2f' % roc_auc)\n",
    "plt.plot(fpr_p, tpr_p, 'g', label = 'polynomial AUC = %0.2f' % roc_auc_p)\n",
    "plt.plot(fpr_k, tpr_k, 'r', label = 'knn AUC = %0.2f' % roc_auc_k)\n",
    "plt.plot(fpr_g, tpr_g, 'y', label = 'gbm AUC = %0.2f' % roc_auc_g)\n",
    "plt.plot(fpr_s, tpr_s, 'grey', label = 'svm AUC = %0.2f' % roc_auc_s)\n",
    "\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "5e90b38eb84cde50b5eaea3897746e3537bf8205751844c2b0fe51c3f12d8e0c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
