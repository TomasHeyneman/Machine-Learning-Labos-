{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Hotel bookings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this labo, we will exercice cleaning data on a dataset called 'Hotel Bookings'. The Hotel Booking demand dataset contains booking information for a city hotel and a resort hotel. It includes information such as booking time, length of stay, number of adults, children/babies, number of available parking spaces, among other things.\n",
    "The City Hotel from this dataset asked us to predict which clients will cancel. We will do the prediction in a later labo.\n",
    "\n",
    "Some of the code is already given. If you see any '...', then you need to add the right code yourself. Towards the end of the labo, less and less of the code will be given in advance and you will have to completely write the code yourself. If you get stuck, take a look at the code in the demo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Loading packages and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn-darkgrid')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('hotel_bookings.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) The basics\n",
    "First, you have to explore the data. Find out the following things:\n",
    "1. Number of features\n",
    "2. Number of observations\n",
    "3. The different datatypes that can be found in the dataset + the number of features per datatype\n",
    "4. The number of features that have missing data\n",
    "5. The amount of missing data for the features that have missing data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. and 2: number of features and observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. and 5. Missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()[df.isnull().sum() != 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or in percentages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(119390- df.isnull().sum()[df.isnull().sum() != 0])/119390*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) Look at the data\n",
    "Look at the first 5 rows of the data.You won't be able to see all features at once.\n",
    "1. Do the columns and values make sense?\n",
    "2. Look at the features.\n",
    "    + Are there any features that are categorical, but not yet declared an object? If so, tell python.\n",
    "3. There are dates in this dataset. We didn't see yet how to handle dates. \n",
    "    + If the dates are already split up into different features (day-month-year), these can be left as is (except for maybe making the months categorical)\n",
    "    + If the full date is given in one feature\n",
    "        + you have to first convert the feature type to a data, using pd.to_datetime(feature)\n",
    "        + Then you have to extract the day, week, month  and year\n",
    "            + feature.dt.year\n",
    "            + feature.dt.month\n",
    "            + feature.dt.week\n",
    "            + feature.dt.day\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Look at the data - does it make sense?\n",
    "\n",
    "First take a look at feature 0 to 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.iloc[:,0:16].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then to feature 16 to 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[:,16:32].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Convert categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['agent'] = df['agent'].astype('object')\n",
    "df['company'] = df['company'].astype('object')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 119390 entries, 0 to 119389\n",
      "Data columns (total 32 columns):\n",
      " #   Column                          Non-Null Count   Dtype  \n",
      "---  ------                          --------------   -----  \n",
      " 0   hotel                           119390 non-null  object \n",
      " 1   is_canceled                     119390 non-null  int64  \n",
      " 2   lead_time                       119390 non-null  int64  \n",
      " 3   arrival_date_year               119390 non-null  int64  \n",
      " 4   arrival_date_month              119390 non-null  object \n",
      " 5   arrival_date_week_number        119390 non-null  int64  \n",
      " 6   arrival_date_day_of_month       119390 non-null  int64  \n",
      " 7   stays_in_weekend_nights         119390 non-null  int64  \n",
      " 8   stays_in_week_nights            119390 non-null  int64  \n",
      " 9   adults                          119390 non-null  int64  \n",
      " 10  children                        119386 non-null  float64\n",
      " 11  babies                          119390 non-null  int64  \n",
      " 12  meal                            119390 non-null  object \n",
      " 13  country                         118902 non-null  object \n",
      " 14  market_segment                  119390 non-null  object \n",
      " 15  distribution_channel            119390 non-null  object \n",
      " 16  is_repeated_guest               119390 non-null  int64  \n",
      " 17  previous_cancellations          119390 non-null  int64  \n",
      " 18  previous_bookings_not_canceled  119390 non-null  int64  \n",
      " 19  reserved_room_type              119390 non-null  object \n",
      " 20  assigned_room_type              119390 non-null  object \n",
      " 21  booking_changes                 119390 non-null  int64  \n",
      " 22  deposit_type                    119390 non-null  object \n",
      " 23  agent                           103050 non-null  object \n",
      " 24  company                         6797 non-null    object \n",
      " 25  days_in_waiting_list            119390 non-null  int64  \n",
      " 26  customer_type                   119390 non-null  object \n",
      " 27  adr                             119390 non-null  float64\n",
      " 28  required_car_parking_spaces     119390 non-null  int64  \n",
      " 29  total_of_special_requests       119390 non-null  int64  \n",
      " 30  reservation_status              119390 non-null  object \n",
      " 31  reservation_status_date         119390 non-null  object \n",
      "dtypes: float64(2), int64(16), object(14)\n",
      "memory usage: 29.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Deal with features that are dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['reservation_status_date'] = pd.to_datetime(df['reservation_status_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['reservation_status_date_year'] = df['reservation_status_date'].dt.year\n",
    "df['reservation_status_date_month'] = df['reservation_status_date'].dt.month\n",
    "df['reservation_status_date_week'] = df['reservation_status_date'].dt.week\n",
    "df['reservation_status_date_day'] = df['reservation_status_date'].dt.day\n",
    "df = df.drop(['reservation_status_date'],1)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c) Look at the descriptives\n",
    "1. For which features do you suspect outliers?\n",
    "2. Which of these outliers seem most suspicious? Which would you certainly check if you were able to?\n",
    "3. For which years do you have an arrival date?\n",
    "4. For which years do you have a reservation date?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First take a look at feature 0 to 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.iloc[:,0:18].describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then to feature 18 to 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.iloc[:,18:35].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d) Plot the distributions\n",
    "1. Make histograms for the numerical features\n",
    "2. Make barplots for the categorical features: which features have sparse classes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.hist( figsize = (12,14))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "categorical = ['hotel','arrival_date_month','meal','country','market_segment','distribution_channel','reserved_room_type', \n",
    "'assigned_room_type','deposit_type','agent','company','customer_type','reservation_status']\n",
    "for i in categorical: \n",
    "    sns.countplot(y=df[i])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## e) Study the associations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Make crosstabs between the outcome 'is_canceled' and the categorical features. Use the option normalize='index' to get proportions, \n",
    "    1. Which customer_type cancels the most? \n",
    "    2. Which hotel has most cancellations?\n",
    "    3. Which room type always cancels? How can you explain this?\n",
    "    4. What do you notice for the feature 'reservation_status'. What will you do with this feature? Does this  have any consequences for other features?\n",
    "2. Make a heat map for all continuous features    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in categorical: \n",
    "    print(pd.crosstab(df[i],df['is_canceled'], normalize='index'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corrmat = df.corr()\n",
    "f, ax = plt.subplots(figsize=(8, 8))\n",
    "sns.heatmap(corrmat, cmap='RdYlGn');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the data cleaning step, we will fix most of the issues that we noticed during the exploratory analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) Remove unwanted observations and features\n",
    "1. Remove duplicated observations\n",
    "2. Remove irrelevant observations (hint: for which hotel do you work?):\n",
    "3. Remove irrelevant/leaky features (hint: what did you notice during the previous excercice?)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Remove duplicated observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_no_dup = df.copy()\n",
    "df_no_dup.drop_duplicates()   # Remove the duplicates\n",
    "print(df_no_dup.shape)               # check the remaining number of observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Remove irrelevant observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_city = df_no_dup.copy()\n",
    "df_city = df_no_dup[df_no_dup['hotel'] == 'City Hotel']\n",
    "df_city.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Remove irrelevant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_city = df_city.drop(['hotel','reservation_status','reservation_status_date_year', 'reservation_status_date_month',    # insert the features that you want to drop\n",
    "                       'reservation_status_date_week', 'reservation_status_date_day'],1)\n",
    "df_city.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) Fix structural errors\n",
    "\n",
    "Unless you found any structural errors, you can skip this step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c) Deal with unwanted outliers\n",
    "1. Do you need to remove any outliers?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d) Handle missing data\n",
    "1. Find out which features still have missing data now that we only have the observations of the city hotel.\n",
    "2. Impute the missing values of the continuous feature(s).\n",
    "3. Is it a good idea to flag the missigness here?\n",
    "4. Appoint the missing values to an extra category 'Undefined'. You will have to define the features as categorical first.\n",
    "    + Why 'Undefined' instead of the usual 'Missing?'\n",
    "5. Check if all the missingness is gone    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Finding missingness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_city.isnull().sum()[df_city.isnull().sum() != 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. continuous feature(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imputed = df_city.copy()\n",
    "\n",
    "# import Imputer \n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Create an imputer object that looks for 'Nan' values, then replaces them with the mean value of the feature by columns (axis=0)\n",
    "mean_imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "\n",
    "# Train the imputor on the dataset\n",
    "mean_imputer = mean_imputer.fit(np.array(df_city['children']).reshape(-1, 1) )\n",
    "\n",
    "# Apply the imputer to the dataset (This imputer can also be used on future datasets)\n",
    "df_imputed['children'] = mean_imputer.transform(np.array(df_city['children']).reshape(-1, 1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imputed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical = ['agent','company', 'country']\n",
    "\n",
    "\n",
    "for col in categorical:\n",
    "    df_imputed[col] = df_imputed[col].astype('category')\n",
    "    df_imputed[col] = df_imputed[col].cat.add_categories('Undefined')\n",
    "    df_imputed[col] = df_imputed[col].fillna('Undefined')\n",
    "    df_imputed[col] = df_imputed[col].astype('object')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imputed.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature engineering\n",
    "\n",
    "\n",
    "## a) Create new features\n",
    "\n",
    "1. Create a new feature 'length_stays', by combining the information in 'stays_in_weekend_nights' and 'stays_in_week_nights'\n",
    "2. Create a new feature 'total_previous_bookings'. Which features will you combine for this?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imputed['length_stays'] = df_imputed['stays_in_weekend_nights'] + df_imputed['stays_in_week_nights']\n",
    "df_imputed['total_previous_bookings'] = df_imputed['previous_cancellations'] + df_imputed['previous_bookings_not_canceled']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) Combine sparse classes\n",
    "\n",
    "We had eleven categorical features with sparse classes.\n",
    "1. Remake the barplots for these features.\n",
    "2. combine the sparse classes\n",
    "    + meal: combine 'FB', and 'HB' into 'FB/HB'\n",
    "    + country: combine all countries with less than 1% of the total observations into 'Other'\n",
    "    + market_segment: Combine 'Aviation', 'Corporate', 'Complementary', and 'Undefined' into 'Other'  \n",
    "    + distribution_channel: Combine 'Corporate', 'GDS' and 'Undefined' into 'Other' \n",
    "    + reserved_room_type: Combine 'B', 'E', 'F', 'G','C','P'  into 'Other\n",
    "    + assigned_room_type: Combine 'B', 'E', 'F','G','C','P' and 'K' into 'Other\n",
    "    + deposit_type: combine 'Non Refund', and 'Refundable' into 'Deposit'\n",
    "    + agent: combine all agents with less than 1% of the total observations into 'Other'\n",
    "    + company: combine all companies with less than 1% of the total observations into 'Other'\n",
    "    + customer_type: combine 'Contract' and 'Group' into 'Other'\n",
    " 3. Check the changes by remaking the plots   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. New plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical = ['meal','country','market_segment','distribution_channel','reserved_room_type', \n",
    "'assigned_room_type','deposit_type','agent','company','customer_type']\n",
    "for i in categorical:\n",
    "    sns.countplot(y=df_imputed[i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Combining sparse classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sparse = df_imputed.copy()\n",
    "\n",
    "# meal\n",
    "df_sparse['meal'][df_sparse['meal'] == 'HB'] = 'HB/FB'\n",
    "df_sparse['meal'][df_sparse['meal'] == 'FB'] = 'HB/FB'\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#country\n",
    "threshold_percent = 1\n",
    "\n",
    "series = pd.value_counts(df_sparse['country'])\n",
    "mask = (series / series.sum() * 100).lt(threshold_percent)\n",
    "df_sparse['country']= np.where(df_sparse['country'].isin(series[mask].index),'Other', df_sparse['country'])\n",
    "df_sparse['country'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# market_segment\n",
    "df_sparse['market_segment'][df_sparse['market_segment'] == 'Complementary'] = 'Other'\n",
    "df_sparse['market_segment'][df_sparse['market_segment'] == 'Aviation'] = 'Other'\n",
    "df_sparse['market_segment'][df_sparse['market_segment'] == 'Undefined'] = 'Other'\n",
    "df_sparse['market_segment'][df_sparse['market_segment'] == 'Corpoeate'] = 'Corpoeate'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution_channel\n",
    "df_sparse['distribution_channel'][df_sparse['distribution_channel'] == 'Corporate'] = 'Other'\n",
    "df_sparse['distribution_channel'][df_sparse['distribution_channel'] == 'GDS'] = 'Other'\n",
    "df_sparse['distribution_channel'][df_sparse['distribution_channel'] == 'Undefined'] = 'Other'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reserved_room_type\n",
    "df_sparse['reserved_room_type'][df_sparse['reserved_room_type'] == 'B'] = 'Other'\n",
    "df_sparse['reserved_room_type'][df_sparse['reserved_room_type'] == 'E'] = 'Other'\n",
    "df_sparse['reserved_room_type'][df_sparse['reserved_room_type'] == 'F'] = 'Other'\n",
    "\n",
    "df_sparse['reserved_room_type'][df_sparse['reserved_room_type'] == 'G'] = 'Other'\n",
    "df_sparse['reserved_room_type'][df_sparse['reserved_room_type'] == 'C'] = 'Other'\n",
    "df_sparse['reserved_room_type'][df_sparse['reserved_room_type'] == 'P'] = 'Other'\n",
    "\n",
    "# assigned_room_type\n",
    "df_sparse['assigned_room_type'][df_sparse['assigned_room_type'] == 'B'] = 'Other'\n",
    "df_sparse['assigned_room_type'][df_sparse['assigned_room_type'] == 'E'] = 'Other'\n",
    "df_sparse['assigned_room_type'][df_sparse['assigned_room_type'] == 'F'] = 'Other'\n",
    "\n",
    "df_sparse['assigned_room_type'][df_sparse['assigned_room_type'] == 'G'] = 'Other'\n",
    "df_sparse['assigned_room_type'][df_sparse['assigned_room_type'] == 'C'] = 'Other'\n",
    "df_sparse['assigned_room_type'][df_sparse['assigned_room_type'] == 'P'] = 'Other'\n",
    "df_sparse['assigned_room_type'][df_sparse['assigned_room_type'] == 'K'] = 'Other'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deposit_type\n",
    "df_sparse['deposit_type'][df_sparse['deposit_type'] == 'Non Refund'] = 'Deposit'\n",
    "df_sparse['deposit_type'][df_sparse['deposit_type'] == 'Refundable'] = 'Deposit'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#agent\n",
    "threshold_percent = 1\n",
    "\n",
    "series = pd.value_counts(df_sparse['agent'])\n",
    "mask = (series / series.sum() * 100).lt(threshold_percent)\n",
    "df_sparse['agent']= np.where(df_sparse['agent'].isin(series[mask].index),'Other', df_sparse['agent'])\n",
    "df_sparse['agent'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#company\n",
    "threshold_percent = 1\n",
    "\n",
    "series = pd.value_counts(df_sparse['company'])\n",
    "mask = (series / series.sum() * 100).lt(threshold_percent)\n",
    "df_sparse['company']= np.where(df_sparse['company'].isin(series[mask].index),'Other', df_sparse['company'])\n",
    "df_sparse['company'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Customer type\n",
    "threshold_percent = 1\n",
    "\n",
    "series = pd.value_counts(df_sparse['customer_type'])\n",
    "mask = (series / series.sum() * 100).lt(threshold_percent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical = ['meal','country','market_segment','distribution_channel',\n",
    "'reserved_room_type','assigned_room_type','deposit_type','agent','company','customer_type']\n",
    "for i in categorical:\n",
    "    sns.countplot(y=df_sparse[i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c) Create dummy variables (categorical features)\n",
    "1. Use one-hot-encoding for all categorical features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sparse = df_sparse.drop(['country','agent','market_segment'],1)\n",
    "\n",
    "\n",
    "for col in df_sparse:\n",
    "    if df_sparse[col].dtype ==  'object':\n",
    "        dummies = pd.get_dummies(df_sparse[col], dummy_na=False, prefix=col)  #create dummies\n",
    "        df_sparse = pd.concat([df_sparse, dummies],axis=1)                   # add dummies to dataset\n",
    "        df_sparse.drop(columns=[col], inplace=True)                           # delete original feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d) Do log-transformation if necessary\n",
    "1. Do log-transformations on the skewed features\n",
    "    + Find out which features are numeric\n",
    "    + But first, change the type of 'is_repeated_guest' to type 'category'\n",
    "    + Calculate the skewness of all features\n",
    "    + Keep the features with skewness larger than 0.75, get their index and take the log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this feature to ype 'category'\n",
    "df_sparse['is_repeated_guest'] = df_sparse['is_repeated_guest'].astype('category')\n",
    "\n",
    "# Select the features with type 'int64' and 'float64'\n",
    "num_feat = df_sparse.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import skew\n",
    "\n",
    "# calculate the skewness\n",
    "skewed = df_sparse[num_feat].apply(lambda x: skew(x.dropna().astype(float)))\n",
    "\n",
    "# Only keep features where skewness is larger than 0.75\n",
    "skewed = skewed[skewed > 0.75]\n",
    "\n",
    "# Get the indexes of these features\n",
    "skewed = skewed.index\n",
    "\n",
    "# Take the log\n",
    "df_sparse[skewed] = np.log1p(df_sparse[skewed])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing cleaned dataset to use in class 3\n",
    "df_sparse_save = df_sparse.copy()\n",
    "df_sparse_save = df_sparse_save.sample(n=10000)\n",
    "df_sparse_save.to_csv('hotel_bookings_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## e) Standardize continuous features\n",
    "1. Standardize the continuous features using \n",
    "    + StandardScaler\n",
    "    + MinMaxSCaler\n",
    "    + RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# standardisation\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df_sparse[num_feat])\n",
    "df_sparse[num_feat] = scaler.transform(df_sparse[num_feat])\n",
    "\n",
    "\n",
    "#MinMaxScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df_sparse[num_feat])\n",
    "df_sparse[num_feat] = scaler.transform(df_sparse[num_feat])\n",
    "\n",
    "\n",
    "#Robustscaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "scaler = RobustScaler()\n",
    "scaler.fit(df_sparse[num_feat])\n",
    "df_sparse[num_feat] = scaler.transform(df_sparse[num_feat])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now you know all the basics on how to clean a dataset. Let's try it on the data from your project!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_project-P1P5ZuzO",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "f61db09535c58f339c125d509803b6419a599ee40ce365314274e78494f92827"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
